{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the code for text preprocessing and cleaning the extracted tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the training and testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_amazon=pd.read_csv('AmazonMaster.csv')\n",
    "tweets_amazon.drop(tweets_amazon.columns[0],inplace=True,axis=1)\n",
    "#tweets_amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read tesla data\n",
    "tweets_tesla=pd.read_csv('TeslaMaster.csv')\n",
    "tweets_tesla.drop(tweets_tesla.columns[0],inplace=True,axis=1)\n",
    "#tweets_tesla.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read apple data\n",
    "df_apple=pd.read_csv(\"AppleTweets.csv\")\n",
    "df_apple.drop(df_apple.columns[0],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read uber data\n",
    "df_uber=pd.read_csv(r\"Uber_3000.csv\", encoding='cp1252')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove words starting with @\n",
    "def remove_symbolA(a):\n",
    "    return(\" \".join(filter(lambda x:x[0]!='@', a.split())))\n",
    "\n",
    "#remove words starting with #\n",
    "def remove_symbolB(b):\n",
    "    return(\" \".join(filter(lambda x:x[0]!='#', b.split())))\n",
    "\n",
    "#convert a list to string\n",
    "def listToString(s): \n",
    "    listToStr = ' '.join([str(elem) for elem in s])\n",
    "    return(listToStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the train and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing @AmazonHelp tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a subset of data for text preprocessing\n",
    "df_amazon = tweets_amazon.iloc[:, [1,2]]\n",
    "#removing words starting with @\n",
    "df_amazon['Text']=df_amazon['Text'].apply(remove_symbolA)\n",
    "#remove words staring with #\n",
    "df_amazon['Text']=df_amazon['Text'].apply(remove_symbolB)\n",
    "#Expanding Contractions\n",
    "df_amazon['no_contract'] = df_amazon['Text'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "#converting list back into a string\n",
    "df_amazon['no_contract'] = [' '.join(map(str, l)) for l in df_amazon['no_contract']]\n",
    "#tokenizing\n",
    "df_amazon['tokenized'] = df_amazon['no_contract'].apply(word_tokenize)\n",
    "#converting to lower case\n",
    "df_amazon['lower_case'] = df_amazon['tokenized'].apply(lambda x: [word.lower() for word in x])\n",
    "#removing punctuation\n",
    "punc = string.punctuation\n",
    "df_amazon['punctuation_removed'] = df_amazon['lower_case'].apply(lambda x: [word for word in x if word not in punc])\n",
    "#removing stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "newStopWords = ['https','http','hi','hey','hello']\n",
    "stop_words.extend(newStopWords)\n",
    "df_amazon['stopwords_removed'] = df_amazon['punctuation_removed'] .apply(lambda x: [word for word in x if word not in stop_words])\n",
    "#converting list back to string\n",
    "df_amazon['Text_Cleaned'] =df_amazon['stopwords_removed'].apply(listToString)\n",
    "#adding the target variable\n",
    "df_amazon[\"Target\"]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the dataframe to csv file\n",
    "df_amazon.to_csv('AmazonHelp_CleanedData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing @Tesla tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a subset of data for text preprocessing\n",
    "df_tesla = tweets_tesla.iloc[:, [1,2]]\n",
    "#removing words starting with @\n",
    "df_tesla['Text']=df_tesla['Text'].apply(remove_symbolA)\n",
    "#remove words staring with #\n",
    "df_tesla['Text']=df_tesla['Text'].apply(remove_symbolB)\n",
    "#Expanding Contractions\n",
    "df_tesla['no_contract'] = df_tesla['Text'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "#converting list back into a string\n",
    "df_tesla['no_contract'] = [' '.join(map(str, l)) for l in df_tesla['no_contract']]\n",
    "#tokenizing\n",
    "df_tesla['tokenized'] = df_tesla['no_contract'].apply(word_tokenize)\n",
    "#converting to lower case\n",
    "df_tesla['lower_case'] = df_tesla['tokenized'].apply(lambda x: [word.lower() for word in x])\n",
    "#removing punctuation\n",
    "punc = string.punctuation\n",
    "df_tesla['punctuation_removed'] = df_tesla['lower_case'].apply(lambda x: [word for word in x if word not in punc])\n",
    "#removing stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "newStopWords = ['tesla','tsla','http','https', 'co']\n",
    "stop_words.extend(newStopWords)\n",
    "df_tesla['stopwords_removed'] = df_tesla['punctuation_removed'] .apply(lambda x: [word for word in x if word not in stop_words])\n",
    "#converting list back to string\n",
    "df_tesla['Text_Cleaned'] =df_tesla['stopwords_removed'].apply(listToString)\n",
    "#adding the target variable\n",
    "df_tesla[\"Target\"]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tesla.to_csv('Tesla_CleanedData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing @Uber tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_apple.shape\n",
    "#since this dataframe has 150000 rows we will randomly sample \n",
    "#and take 1500 rows only. This is done to ensure that the output classes are balanced\n",
    "df_apple=df_apple.sample(frac = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a subset of data for text preprocessing\n",
    "df_apple = df_apple.iloc[:, [0,7]]\n",
    "#removing words starting with @\n",
    "df_apple['text']=df_apple['text'].apply(remove_symbolA)\n",
    "#remove words staring with #\n",
    "df_apple['text']=df_apple['text'].apply(remove_symbolB)\n",
    "#Expanding Contractions\n",
    "df_apple['no_contract'] = df_apple['text'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "#converting list back into a string\n",
    "df_apple['no_contract'] = [' '.join(map(str, l)) for l in df_apple['no_contract']]\n",
    "#tokenizing\n",
    "df_apple['tokenized'] = df_apple['no_contract'].apply(word_tokenize)\n",
    "#converting to lower case\n",
    "df_apple['lower_case'] = df_apple['tokenized'].apply(lambda x: [word.lower() for word in x])\n",
    "#removing punctuation\n",
    "punc = string.punctuation\n",
    "df_apple['punctuation_removed'] = df_apple['lower_case'].apply(lambda x: [word for word in x if word not in punc])\n",
    "#removing stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "newStopWords = ['https','http','hi','hey','hello']\n",
    "stop_words.extend(newStopWords)\n",
    "df_apple['stopwords_removed'] = df_apple['punctuation_removed'] .apply(lambda x: [word for word in x if word not in stop_words])\n",
    "#converting list back to string\n",
    "df_apple['Text_Cleaned'] =df_apple['stopwords_removed'].apply(listToString)\n",
    "#adding the target variable\n",
    "df_apple[\"Target\"]=0\n",
    "#changing column name\n",
    "df_apple.rename(columns = {\"text\":\"Text\",\"id\":\"Tweet Id\"},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_apple.to_csv('Apple_CleanedData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing @Uber tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a subset of data for text preprocessing\n",
    "df_uber = df_uber.iloc[:, [0,1,6,7,9]]\n",
    "#removing words starting with @\n",
    "df_uber['Text']=df_uber['Text'].apply(remove_symbolA)\n",
    "#remove words staring with #\n",
    "df_uber['Text']=df_uber['Text'].apply(remove_symbolB)\n",
    "#Expanding Contractions\n",
    "df_uber['no_contract'] = df_uber['Text'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "#converting list back into a string\n",
    "df_uber['no_contract'] = [' '.join(map(str, l)) for l in df_uber['no_contract']]\n",
    "#tokenizing\n",
    "df_uber['tokenized'] = df_uber['no_contract'].apply(word_tokenize)\n",
    "#converting to lower case\n",
    "df_uber['lower_case'] = df_uber['tokenized'].apply(lambda x: [word.lower() for word in x])\n",
    "#removing punctuation\n",
    "punc = string.punctuation\n",
    "df_uber['punctuation_removed'] = df_uber['lower_case'].apply(lambda x: [word for word in x if word not in punc])\n",
    "#removing stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "newStopWords = ['https','http','hi','hey','hello']\n",
    "stop_words.extend(newStopWords)\n",
    "df_uber['stopwords_removed'] = df_uber['punctuation_removed'] .apply(lambda x: [word for word in x if word not in stop_words])\n",
    "#converting list back to string\n",
    "df_uber['Text_Cleaned'] =df_uber['stopwords_removed'].apply(listToString)\n",
    "#adding the target variable\n",
    "df_uber[\"Target\"]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uber.to_csv('Uber_CleanedData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
